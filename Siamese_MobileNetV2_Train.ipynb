{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import cuda\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.models.mobilenet import mobilenet_v2\n",
    "\n",
    "# DLBio and own scripts\n",
    "from DLBio.pytorch_helpers import get_device\n",
    "import ds_ear_siamese\n",
    "import transforms_data as td\n",
    "from helpers import cuda_conv\n",
    "import metrics as M\n",
    "from siamese_network_train import Training\n",
    "from ContrastiveLossFunction import ContrastiveLoss\n",
    "from NN_Siamese import SiameseNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img,text=None,should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()    \n",
    "\n",
    "def show_plot(epochs, value1, value2, label1, label2, plt_number):\n",
    "    plt.figure(plt_number)\n",
    "    plt.plot(epochs,value1, label=label1)\n",
    "    plt.plot(epochs,value2, label=label2)\n",
    "    plt.legend()\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up All Configurations here\n",
    "class Config():\n",
    "    #1. Boolean Ã¤ndern\n",
    "    #2. FC-Layer auf Bild anpassen\n",
    "    #3. LR auf 0,0005\n",
    "    NN_SIAMESE = False\n",
    "    USE_SAVED_MODEL = True\n",
    "    dataset_dir = '../dataset/'\n",
    "    # training_dir = \"../data/ears/training/\"\n",
    "    # testing_dir = \"../data/ears/testing/\"\n",
    "    train_batch_size = 16\n",
    "    val_batch_size = 16\n",
    "    test_batch_size = 16\n",
    "    vis_batch_size = 8\n",
    "    num_workers = 3\n",
    "    \n",
    "    EPOCHS= 30\n",
    "    LEARNINGRATE = 0.001\n",
    "    #WEIGHT_DECAY = 0.0\n",
    "\n",
    "    TRESHOLD_VER = 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check(p):\n",
    "    return (p>=0 and p<=1)\n",
    "def percentage_split(num_classes, train_percentage, valid_percentage, test_percentage):\n",
    "    '''\n",
    "    This function return the amount of classes to be chosen, given the actual amount and the percentages for\n",
    "    the different sets\n",
    "    Arguments\n",
    "    ---------\n",
    "    num_classes:    Amount of classes in dataset\n",
    "    train_percentage:   Percentage of one, percentual size of train dataset\n",
    "    valid_percentage:   Percentage of one, percentual size of validation dataset\n",
    "    test_percentage:   Percentage of one, percentual size of test dataset. Can be Zero\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    The amount of classes for train, validation and test dataset\n",
    "    '''\n",
    "    assert round(train_percentage+valid_percentage+test_percentage, 1)==1.0, \"Percentages do not add up to 1\"\n",
    "    assert check(train_percentage) and check(valid_percentage) and check(test_percentage)\n",
    "\n",
    "    train = round(num_classes * train_percentage)\n",
    "    if test_percentage == 0:\n",
    "        valid = num_classes-train\n",
    "        test = test_percentage\n",
    "    else:\n",
    "        valid = round( (num_classes*valid_percentage) )\n",
    "        test = num_classes-train-valid\n",
    "    return train,valid,test\n",
    "\n",
    "def diff(first, second):\n",
    "    ''' computes the list diff\n",
    "    '''\n",
    "    second = set(second)\n",
    "    return [item for item in first if item not in second]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['alexander_bec', 'alina_sch', 'alissa_buh', 'beatrix_mah', 'clara_pau', 'clemens_blu', 'collin_sch', 'david_fau', 'falco_len', 'felix_mec', 'gregor_spi', 'hammam_als', 'janna_qua', 'jesse_kru', 'jule_dre', 'konrad_von', 'lars_fin', 'marcel_nim', 'marina_fri', 'mila_wol', 'mohammed_muh', 'moritz_bor', 'nils_loo', 'robert_kle', 'sarah_feh', 'tim_moe', 'yannik_obe']\n2160\n[3, 20, 15, 0, 7, 14, 9, 21, 16, 13, 1, 8, 17, 10, 19, 22] [24, 11, 12, 26, 2, 4, 18, 6] [25, 23, 5]\n0 0\n1 0\n2 0\n3 0\n4 0\n5 0\n6 0\n7 0\n8 0\n9 0\n10 0\n11 0\n12 0\n13 0\n14 0\n15 0\n16 0\n17 0\n18 0\n19 0\n20 0\n21 0\n22 0\n23 0\n24 0\n25 0\n26 0\n27 0\n28 0\n29 0\n30 0\n31 0\n32 0\n33 0\n34 0\n35 0\n36 0\n37 0\n38 0\n39 0\n40 0\n41 0\n42 0\n43 0\n44 0\n45 0\n46 0\n47 0\n48 0\n49 0\n50 0\n51 0\n52 0\n53 0\n54 0\n55 0\n56 0\n57 0\n58 0\n59 0\n60 0\n61 0\n62 0\n63 0\n64 0\n65 0\n66 0\n67 0\n68 0\n69 0\n70 0\n71 0\n72 0\n73 0\n74 0\n75 0\n76 0\n77 0\n78 0\n79 0\n80 1\n81 1\n82 1\n83 1\n84 1\n85 1\n86 1\n87 1\n88 1\n89 1\n90 1\n91 1\n92 1\n93 1\n94 1\n95 1\n96 1\n97 1\n98 1\n99 1\n100 1\n101 1\n102 1\n103 1\n104 1\n105 1\n106 1\n107 1\n108 1\n109 1\n110 1\n111 1\n112 1\n113 1\n114 1\n115 1\n116 1\n117 1\n118 1\n119 1\n120 1\n121 1\n122 1\n123 1\n124 1\n125 1\n126 1\n127 1\n128 1\n129 1\n130 1\n131 1\n132 1\n133 1\n134 1\n135 1\n136 1\n137 1\n138 1\n139 1\n140 1\n141 1\n142 1\n143 1\n144 1\n145 1\n146 1\n147 1\n148 1\n149 1\n150 1\n151 1\n152 1\n153 1\n154 1\n155 1\n156 1\n157 1\n158 1\n159 1\n160 2\n161 2\n162 2\n163 2\n164 2\n165 2\n166 2\n167 2\n168 2\n169 2\n170 2\n171 2\n172 2\n173 2\n174 2\n175 2\n176 2\n177 2\n178 2\n179 2\n180 2\n181 2\n182 2\n183 2\n184 2\n185 2\n186 2\n187 2\n188 2\n189 2\n190 2\n191 2\n192 2\n193 2\n194 2\n195 2\n196 2\n197 2\n198 2\n199 2\n200 2\n201 2\n202 2\n203 2\n204 2\n205 2\n206 2\n207 2\n208 2\n209 2\n210 2\n211 2\n212 2\n213 2\n214 2\n215 2\n216 2\n217 2\n218 2\n219 2\n220 2\n221 2\n222 2\n223 2\n224 2\n225 2\n226 2\n227 2\n228 2\n229 2\n230 2\n231 2\n232 2\n233 2\n234 2\n235 2\n236 2\n237 2\n238 2\n239 2\n240 3\n241 3\n242 3\n243 3\n244 3\n245 3\n246 3\n247 3\n248 3\n249 3\n250 3\n251 3\n252 3\n253 3\n254 3\n255 3\n256 3\n257 3\n258 3\n259 3\n260 3\n261 3\n262 3\n263 3\n264 3\n265 3\n266 3\n267 3\n268 3\n269 3\n270 3\n271 3\n272 3\n273 3\n274 3\n275 3\n276 3\n277 3\n278 3\n279 3\n280 3\n281 3\n282 3\n283 3\n284 3\n285 3\n286 3\n287 3\n288 3\n289 3\n290 3\n291 3\n292 3\n293 3\n294 3\n295 3\n296 3\n297 3\n298 3\n299 3\n300 3\n301 3\n302 3\n303 3\n304 3\n305 3\n306 3\n307 3\n308 3\n309 3\n310 3\n311 3\n312 3\n313 3\n314 3\n315 3\n316 3\n317 3\n318 3\n319 3\n320 4\n321 4\n322 4\n323 4\n324 4\n325 4\n326 4\n327 4\n328 4\n329 4\n330 4\n331 4\n332 4\n333 4\n334 4\n335 4\n336 4\n337 4\n338 4\n339 4\n340 4\n341 4\n342 4\n343 4\n344 4\n345 4\n346 4\n347 4\n348 4\n349 4\n350 4\n351 4\n352 4\n353 4\n354 4\n355 4\n356 4\n357 4\n358 4\n359 4\n360 4\n361 4\n362 4\n363 4\n364 4\n365 4\n366 4\n367 4\n368 4\n369 4\n370 4\n371 4\n372 4\n373 4\n374 4\n375 4\n376 4\n377 4\n378 4\n379 4\n380 4\n381 4\n382 4\n383 4\n384 4\n385 4\n386 4\n387 4\n388 4\n389 4\n390 4\n391 4\n392 4\n393 4\n394 4\n395 4\n396 4\n397 4\n398 4\n399 4\n400 5\n401 5\n402 5\n403 5\n404 5\n405 5\n406 5\n407 5\n408 5\n409 5\n410 5\n411 5\n412 5\n413 5\n414 5\n415 5\n416 5\n417 5\n418 5\n419 5\n420 5\n421 5\n422 5\n423 5\n424 5\n425 5\n426 5\n427 5\n428 5\n429 5\n430 5\n431 5\n432 5\n433 5\n434 5\n435 5\n436 5\n437 5\n438 5\n439 5\n440 5\n441 5\n442 5\n443 5\n444 5\n445 5\n446 5\n447 5\n448 5\n449 5\n450 5\n451 5\n452 5\n453 5\n454 5\n455 5\n456 5\n457 5\n458 5\n459 5\n460 5\n461 5\n462 5\n463 5\n464 5\n465 5\n466 5\n467 5\n468 5\n469 5\n470 5\n471 5\n472 5\n473 5\n474 5\n475 5\n476 5\n477 5\n478 5\n479 5\n480 6\n481 6\n482 6\n483 6\n484 6\n485 6\n486 6\n487 6\n488 6\n489 6\n490 6\n491 6\n492 6\n493 6\n494 6\n495 6\n496 6\n497 6\n498 6\n499 6\n500 6\n501 6\n502 6\n503 6\n504 6\n505 6\n506 6\n507 6\n508 6\n509 6\n510 6\n511 6\n512 6\n513 6\n514 6\n515 6\n516 6\n517 6\n518 6\n519 6\n520 6\n521 6\n522 6\n523 6\n524 6\n525 6\n526 6\n527 6\n528 6\n529 6\n530 6\n531 6\n532 6\n533 6\n534 6\n535 6\n536 6\n537 6\n538 6\n539 6\n540 6\n541 6\n542 6\n543 6\n544 6\n545 6\n546 6\n547 6\n548 6\n549 6\n550 6\n551 6\n552 6\n553 6\n554 6\n555 6\n556 6\n557 6\n558 6\n559 6\n560 7\n561 7\n562 7\n563 7\n564 7\n565 7\n566 7\n567 7\n568 7\n569 7\n570 7\n571 7\n572 7\n573 7\n574 7\n575 7\n576 7\n577 7\n578 7\n579 7\n580 7\n581 7\n582 7\n583 7\n584 7\n585 7\n586 7\n587 7\n588 7\n589 7\n590 7\n591 7\n592 7\n593 7\n594 7\n595 7\n596 7\n597 7\n598 7\n599 7\n600 7\n601 7\n602 7\n603 7\n604 7\n605 7\n606 7\n607 7\n608 7\n609 7\n610 7\n611 7\n612 7\n613 7\n614 7\n615 7\n616 7\n617 7\n618 7\n619 7\n620 7\n621 7\n622 7\n623 7\n624 7\n625 7\n626 7\n627 7\n628 7\n629 7\n630 7\n631 7\n632 7\n633 7\n634 7\n635 7\n636 7\n637 7\n638 7\n639 7\n640 8\n641 8\n642 8\n643 8\n644 8\n645 8\n646 8\n647 8\n648 8\n649 8\n650 8\n651 8\n652 8\n653 8\n654 8\n655 8\n656 8\n657 8\n658 8\n659 8\n660 8\n661 8\n662 8\n663 8\n664 8\n665 8\n666 8\n667 8\n668 8\n669 8\n670 8\n671 8\n672 8\n673 8\n674 8\n675 8\n676 8\n677 8\n678 8\n679 8\n680 8\n681 8\n682 8\n683 8\n684 8\n685 8\n686 8\n687 8\n688 8\n689 8\n690 8\n691 8\n692 8\n693 8\n694 8\n695 8\n696 8\n697 8\n698 8\n699 8\n700 8\n701 8\n702 8\n703 8\n704 8\n705 8\n706 8\n707 8\n708 8\n709 8\n710 8\n711 8\n712 8\n713 8\n714 8\n715 8\n716 8\n717 8\n718 8\n719 8\n720 9\n721 9\n722 9\n723 9\n724 9\n725 9\n726 9\n727 9\n728 9\n729 9\n730 9\n731 9\n732 9\n733 9\n734 9\n735 9\n736 9\n737 9\n738 9\n739 9\n740 9\n741 9\n742 9\n743 9\n744 9\n745 9\n746 9\n747 9\n748 9\n749 9\n750 9\n751 9\n752 9\n753 9\n754 9\n755 9\n756 9\n757 9\n758 9\n759 9\n760 9\n761 9\n762 9\n763 9\n764 9\n765 9\n766 9\n767 9\n768 9\n769 9\n770 9\n771 9\n772 9\n773 9\n774 9\n775 9\n776 9\n777 9\n778 9\n779 9\n780 9\n781 9\n782 9\n783 9\n784 9\n785 9\n786 9\n787 9\n788 9\n789 9\n790 9\n791 9\n792 9\n793 9\n794 9\n795 9\n796 9\n797 9\n798 9\n799 9\n800 10\n801 10\n802 10\n803 10\n804 10\n805 10\n806 10\n807 10\n808 10\n809 10\n810 10\n811 10\n812 10\n813 10\n814 10\n815 10\n816 10\n817 10\n818 10\n819 10\n820 10\n821 10\n822 10\n823 10\n824 10\n825 10\n826 10\n827 10\n828 10\n829 10\n830 10\n831 10\n832 10\n833 10\n834 10\n835 10\n836 10\n837 10\n838 10\n839 10\n840 10\n841 10\n842 10\n843 10\n844 10\n845 10\n846 10\n847 10\n848 10\n849 10\n850 10\n851 10\n852 10\n853 10\n854 10\n855 10\n856 10\n857 10\n858 10\n859 10\n860 10\n861 10\n862 10\n863 10\n864 10\n865 10\n866 10\n867 10\n868 10\n869 10\n870 10\n871 10\n872 10\n873 10\n874 10\n875 10\n876 10\n877 10\n878 10\n879 10\n880 11\n881 11\n882 11\n883 11\n884 11\n885 11\n886 11\n887 11\n888 11\n889 11\n890 11\n891 11\n892 11\n893 11\n894 11\n895 11\n896 11\n897 11\n898 11\n899 11\n900 11\n901 11\n902 11\n903 11\n904 11\n905 11\n906 11\n907 11\n908 11\n909 11\n910 11\n911 11\n912 11\n913 11\n914 11\n915 11\n916 11\n917 11\n918 11\n919 11\n920 11\n921 11\n922 11\n923 11\n924 11\n925 11\n926 11\n927 11\n928 11\n929 11\n930 11\n931 11\n932 11\n933 11\n934 11\n935 11\n936 11\n937 11\n938 11\n939 11\n940 11\n941 11\n942 11\n943 11\n944 11\n945 11\n946 11\n947 11\n948 11\n949 11\n950 11\n951 11\n952 11\n953 11\n954 11\n955 11\n956 11\n957 11\n958 11\n959 11\n960 12\n961 12\n962 12\n963 12\n964 12\n965 12\n966 12\n967 12\n968 12\n969 12\n970 12\n971 12\n972 12\n973 12\n974 12\n975 12\n976 12\n977 12\n978 12\n979 12\n980 12\n981 12\n982 12\n983 12\n984 12\n985 12\n986 12\n987 12\n988 12\n989 12\n990 12\n991 12\n992 12\n993 12\n994 12\n995 12\n996 12\n997 12\n998 12\n999 12\n1000 12\n1001 12\n1002 12\n1003 12\n1004 12\n1005 12\n1006 12\n1007 12\n1008 12\n1009 12\n1010 12\n1011 12\n1012 12\n1013 12\n1014 12\n1015 12\n1016 12\n1017 12\n1018 12\n1019 12\n1020 12\n1021 12\n1022 12\n1023 12\n1024 12\n1025 12\n1026 12\n1027 12\n1028 12\n1029 12\n1030 12\n1031 12\n1032 12\n1033 12\n1034 12\n1035 12\n1036 12\n1037 12\n1038 12\n1039 12\n1040 13\n1041 13\n1042 13\n1043 13\n1044 13\n1045 13\n1046 13\n1047 13\n1048 13\n1049 13\n1050 13\n1051 13\n1052 13\n1053 13\n1054 13\n1055 13\n1056 13\n1057 13\n1058 13\n1059 13\n1060 13\n1061 13\n1062 13\n1063 13\n1064 13\n1065 13\n1066 13\n1067 13\n1068 13\n1069 13\n1070 13\n1071 13\n1072 13\n1073 13\n1074 13\n1075 13\n1076 13\n1077 13\n1078 13\n1079 13\n1080 13\n1081 13\n1082 13\n1083 13\n1084 13\n1085 13\n1086 13\n1087 13\n1088 13\n1089 13\n1090 13\n1091 13\n1092 13\n1093 13\n1094 13\n1095 13\n1096 13\n1097 13\n1098 13\n1099 13\n1100 13\n1101 13\n1102 13\n1103 13\n1104 13\n1105 13\n1106 13\n1107 13\n1108 13\n1109 13\n1110 13\n1111 13\n1112 13\n1113 13\n1114 13\n1115 13\n1116 13\n1117 13\n1118 13\n1119 13\n1120 14\n1121 14\n1122 14\n1123 14\n1124 14\n1125 14\n1126 14\n1127 14\n1128 14\n1129 14\n1130 14\n1131 14\n1132 14\n1133 14\n1134 14\n1135 14\n1136 14\n1137 14\n1138 14\n1139 14\n1140 14\n1141 14\n1142 14\n1143 14\n1144 14\n1145 14\n1146 14\n1147 14\n1148 14\n1149 14\n1150 14\n1151 14\n1152 14\n1153 14\n1154 14\n1155 14\n1156 14\n1157 14\n1158 14\n1159 14\n1160 14\n1161 14\n1162 14\n1163 14\n1164 14\n1165 14\n1166 14\n1167 14\n1168 14\n1169 14\n1170 14\n1171 14\n1172 14\n1173 14\n1174 14\n1175 14\n1176 14\n1177 14\n1178 14\n1179 14\n1180 14\n1181 14\n1182 14\n1183 14\n1184 14\n1185 14\n1186 14\n1187 14\n1188 14\n1189 14\n1190 14\n1191 14\n1192 14\n1193 14\n1194 14\n1195 14\n1196 14\n1197 14\n1198 14\n1199 14\n1200 15\n1201 15\n1202 15\n1203 15\n1204 15\n1205 15\n1206 15\n1207 15\n1208 15\n1209 15\n1210 15\n1211 15\n1212 15\n1213 15\n1214 15\n1215 15\n1216 15\n1217 15\n1218 15\n1219 15\n1220 15\n1221 15\n1222 15\n1223 15\n1224 15\n1225 15\n1226 15\n1227 15\n1228 15\n1229 15\n1230 15\n1231 15\n1232 15\n1233 15\n1234 15\n1235 15\n1236 15\n1237 15\n1238 15\n1239 15\n1240 15\n1241 15\n1242 15\n1243 15\n1244 15\n1245 15\n1246 15\n1247 15\n1248 15\n1249 15\n1250 15\n1251 15\n1252 15\n1253 15\n1254 15\n1255 15\n1256 15\n1257 15\n1258 15\n1259 15\n1260 15\n1261 15\n1262 15\n1263 15\n1264 15\n1265 15\n1266 15\n1267 15\n1268 15\n1269 15\n1270 15\n1271 15\n1272 15\n1273 15\n1274 15\n1275 15\n1276 15\n1277 15\n1278 15\n1279 15\n1280 16\n1281 16\n1282 16\n1283 16\n1284 16\n1285 16\n1286 16\n1287 16\n1288 16\n1289 16\n1290 16\n1291 16\n1292 16\n1293 16\n1294 16\n1295 16\n1296 16\n1297 16\n1298 16\n1299 16\n1300 16\n1301 16\n1302 16\n1303 16\n1304 16\n1305 16\n1306 16\n1307 16\n1308 16\n1309 16\n1310 16\n1311 16\n1312 16\n1313 16\n1314 16\n1315 16\n1316 16\n1317 16\n1318 16\n1319 16\n1320 16\n1321 16\n1322 16\n1323 16\n1324 16\n1325 16\n1326 16\n1327 16\n1328 16\n1329 16\n1330 16\n1331 16\n1332 16\n1333 16\n1334 16\n1335 16\n1336 16\n1337 16\n1338 16\n1339 16\n1340 16\n1341 16\n1342 16\n1343 16\n1344 16\n1345 16\n1346 16\n1347 16\n1348 16\n1349 16\n1350 16\n1351 16\n1352 16\n1353 16\n1354 16\n1355 16\n1356 16\n1357 16\n1358 16\n1359 16\n1360 17\n1361 17\n1362 17\n1363 17\n1364 17\n1365 17\n1366 17\n1367 17\n1368 17\n1369 17\n1370 17\n1371 17\n1372 17\n1373 17\n1374 17\n1375 17\n1376 17\n1377 17\n1378 17\n1379 17\n1380 17\n1381 17\n1382 17\n1383 17\n1384 17\n1385 17\n1386 17\n1387 17\n1388 17\n1389 17\n1390 17\n1391 17\n1392 17\n1393 17\n1394 17\n1395 17\n1396 17\n1397 17\n1398 17\n1399 17\n1400 17\n1401 17\n1402 17\n1403 17\n1404 17\n1405 17\n1406 17\n1407 17\n1408 17\n1409 17\n1410 17\n1411 17\n1412 17\n1413 17\n1414 17\n1415 17\n1416 17\n1417 17\n1418 17\n1419 17\n1420 17\n1421 17\n1422 17\n1423 17\n1424 17\n1425 17\n1426 17\n1427 17\n1428 17\n1429 17\n1430 17\n1431 17\n1432 17\n1433 17\n1434 17\n1435 17\n1436 17\n1437 17\n1438 17\n1439 17\n1440 18\n1441 18\n1442 18\n1443 18\n1444 18\n1445 18\n1446 18\n1447 18\n1448 18\n1449 18\n1450 18\n1451 18\n1452 18\n1453 18\n1454 18\n1455 18\n1456 18\n1457 18\n1458 18\n1459 18\n1460 18\n1461 18\n1462 18\n1463 18\n1464 18\n1465 18\n1466 18\n1467 18\n1468 18\n1469 18\n1470 18\n1471 18\n1472 18\n1473 18\n1474 18\n1475 18\n1476 18\n1477 18\n1478 18\n1479 18\n1480 18\n1481 18\n1482 18\n1483 18\n1484 18\n1485 18\n1486 18\n1487 18\n1488 18\n1489 18\n1490 18\n1491 18\n1492 18\n1493 18\n1494 18\n1495 18\n1496 18\n1497 18\n1498 18\n1499 18\n1500 18\n1501 18\n1502 18\n1503 18\n1504 18\n1505 18\n1506 18\n1507 18\n1508 18\n1509 18\n1510 18\n1511 18\n1512 18\n1513 18\n1514 18\n1515 18\n1516 18\n1517 18\n1518 18\n1519 18\n1520 19\n1521 19\n1522 19\n1523 19\n1524 19\n1525 19\n1526 19\n1527 19\n1528 19\n1529 19\n1530 19\n1531 19\n1532 19\n1533 19\n1534 19\n1535 19\n1536 19\n1537 19\n1538 19\n1539 19\n1540 19\n1541 19\n1542 19\n1543 19\n1544 19\n1545 19\n1546 19\n1547 19\n1548 19\n1549 19\n1550 19\n1551 19\n1552 19\n1553 19\n1554 19\n1555 19\n1556 19\n1557 19\n1558 19\n1559 19\n1560 19\n1561 19\n1562 19\n1563 19\n1564 19\n1565 19\n1566 19\n1567 19\n1568 19\n1569 19\n1570 19\n1571 19\n1572 19\n1573 19\n1574 19\n1575 19\n1576 19\n1577 19\n1578 19\n1579 19\n1580 19\n1581 19\n1582 19\n1583 19\n1584 19\n1585 19\n1586 19\n1587 19\n1588 19\n1589 19\n1590 19\n1591 19\n1592 19\n1593 19\n1594 19\n1595 19\n1596 19\n1597 19\n1598 19\n1599 19\n1600 20\n1601 20\n1602 20\n1603 20\n1604 20\n1605 20\n1606 20\n1607 20\n1608 20\n1609 20\n1610 20\n1611 20\n1612 20\n1613 20\n1614 20\n1615 20\n1616 20\n1617 20\n1618 20\n1619 20\n1620 20\n1621 20\n1622 20\n1623 20\n1624 20\n1625 20\n1626 20\n1627 20\n1628 20\n1629 20\n1630 20\n1631 20\n1632 20\n1633 20\n1634 20\n1635 20\n1636 20\n1637 20\n1638 20\n1639 20\n1640 20\n1641 20\n1642 20\n1643 20\n1644 20\n1645 20\n1646 20\n1647 20\n1648 20\n1649 20\n1650 20\n1651 20\n1652 20\n1653 20\n1654 20\n1655 20\n1656 20\n1657 20\n1658 20\n1659 20\n1660 20\n1661 20\n1662 20\n1663 20\n1664 20\n1665 20\n1666 20\n1667 20\n1668 20\n1669 20\n1670 20\n1671 20\n1672 20\n1673 20\n1674 20\n1675 20\n1676 20\n1677 20\n1678 20\n1679 20\n1680 21\n1681 21\n1682 21\n1683 21\n1684 21\n1685 21\n1686 21\n1687 21\n1688 21\n1689 21\n1690 21\n1691 21\n1692 21\n1693 21\n1694 21\n1695 21\n1696 21\n1697 21\n1698 21\n1699 21\n1700 21\n1701 21\n1702 21\n1703 21\n1704 21\n1705 21\n1706 21\n1707 21\n1708 21\n1709 21\n1710 21\n1711 21\n1712 21\n1713 21\n1714 21\n1715 21\n1716 21\n1717 21\n1718 21\n1719 21\n1720 21\n1721 21\n1722 21\n1723 21\n1724 21\n1725 21\n1726 21\n1727 21\n1728 21\n1729 21\n1730 21\n1731 21\n1732 21\n1733 21\n1734 21\n1735 21\n1736 21\n1737 21\n1738 21\n1739 21\n1740 21\n1741 21\n1742 21\n1743 21\n1744 21\n1745 21\n1746 21\n1747 21\n1748 21\n1749 21\n1750 21\n1751 21\n1752 21\n1753 21\n1754 21\n1755 21\n1756 21\n1757 21\n1758 21\n1759 21\n1760 22\n1761 22\n1762 22\n1763 22\n1764 22\n1765 22\n1766 22\n1767 22\n1768 22\n1769 22\n1770 22\n1771 22\n1772 22\n1773 22\n1774 22\n1775 22\n1776 22\n1777 22\n1778 22\n1779 22\n1780 22\n1781 22\n1782 22\n1783 22\n1784 22\n1785 22\n1786 22\n1787 22\n1788 22\n1789 22\n1790 22\n1791 22\n1792 22\n1793 22\n1794 22\n1795 22\n1796 22\n1797 22\n1798 22\n1799 22\n1800 22\n1801 22\n1802 22\n1803 22\n1804 22\n1805 22\n1806 22\n1807 22\n1808 22\n1809 22\n1810 22\n1811 22\n1812 22\n1813 22\n1814 22\n1815 22\n1816 22\n1817 22\n1818 22\n1819 22\n1820 22\n1821 22\n1822 22\n1823 22\n1824 22\n1825 22\n1826 22\n1827 22\n1828 22\n1829 22\n1830 22\n1831 22\n1832 22\n1833 22\n1834 22\n1835 22\n1836 22\n1837 22\n1838 22\n1839 22\n1840 23\n1841 23\n1842 23\n1843 23\n1844 23\n1845 23\n1846 23\n1847 23\n1848 23\n1849 23\n1850 23\n1851 23\n1852 23\n1853 23\n1854 23\n1855 23\n1856 23\n1857 23\n1858 23\n1859 23\n1860 23\n1861 23\n1862 23\n1863 23\n1864 23\n1865 23\n1866 23\n1867 23\n1868 23\n1869 23\n1870 23\n1871 23\n1872 23\n1873 23\n1874 23\n1875 23\n1876 23\n1877 23\n1878 23\n1879 23\n1880 23\n1881 23\n1882 23\n1883 23\n1884 23\n1885 23\n1886 23\n1887 23\n1888 23\n1889 23\n1890 23\n1891 23\n1892 23\n1893 23\n1894 23\n1895 23\n1896 23\n1897 23\n1898 23\n1899 23\n1900 23\n1901 23\n1902 23\n1903 23\n1904 23\n1905 23\n1906 23\n1907 23\n1908 23\n1909 23\n1910 23\n1911 23\n1912 23\n1913 23\n1914 23\n1915 23\n1916 23\n1917 23\n1918 23\n1919 23\n1920 24\n1921 24\n1922 24\n1923 24\n1924 24\n1925 24\n1926 24\n1927 24\n1928 24\n1929 24\n1930 24\n1931 24\n1932 24\n1933 24\n1934 24\n1935 24\n1936 24\n1937 24\n1938 24\n1939 24\n1940 24\n1941 24\n1942 24\n1943 24\n1944 24\n1945 24\n1946 24\n1947 24\n1948 24\n1949 24\n1950 24\n1951 24\n1952 24\n1953 24\n1954 24\n1955 24\n1956 24\n1957 24\n1958 24\n1959 24\n1960 24\n1961 24\n1962 24\n1963 24\n1964 24\n1965 24\n1966 24\n1967 24\n1968 24\n1969 24\n1970 24\n1971 24\n1972 24\n1973 24\n1974 24\n1975 24\n1976 24\n1977 24\n1978 24\n1979 24\n1980 24\n1981 24\n1982 24\n1983 24\n1984 24\n1985 24\n1986 24\n1987 24\n1988 24\n1989 24\n1990 24\n1991 24\n1992 24\n1993 24\n1994 24\n1995 24\n1996 24\n1997 24\n1998 24\n1999 24\n2000 25\n2001 25\n2002 25\n2003 25\n2004 25\n2005 25\n2006 25\n2007 25\n2008 25\n2009 25\n2010 25\n2011 25\n2012 25\n2013 25\n2014 25\n2015 25\n2016 25\n2017 25\n2018 25\n2019 25\n2020 25\n2021 25\n2022 25\n2023 25\n2024 25\n2025 25\n2026 25\n2027 25\n2028 25\n2029 25\n2030 25\n2031 25\n2032 25\n2033 25\n2034 25\n2035 25\n2036 25\n2037 25\n2038 25\n2039 25\n2040 25\n2041 25\n2042 25\n2043 25\n2044 25\n2045 25\n2046 25\n2047 25\n2048 25\n2049 25\n2050 25\n2051 25\n2052 25\n2053 25\n2054 25\n2055 25\n2056 25\n2057 25\n2058 25\n2059 25\n2060 25\n2061 25\n2062 25\n2063 25\n2064 25\n2065 25\n2066 25\n2067 25\n2068 25\n2069 25\n2070 25\n2071 25\n2072 25\n2073 25\n2074 25\n2075 25\n2076 25\n2077 25\n2078 25\n2079 25\n2080 26\n2081 26\n2082 26\n2083 26\n2084 26\n2085 26\n2086 26\n2087 26\n2088 26\n2089 26\n2090 26\n2091 26\n2092 26\n2093 26\n2094 26\n2095 26\n2096 26\n2097 26\n2098 26\n2099 26\n2100 26\n2101 26\n2102 26\n2103 26\n2104 26\n2105 26\n2106 26\n2107 26\n2108 26\n2109 26\n2110 26\n2111 26\n2112 26\n2113 26\n2114 26\n2115 26\n2116 26\n2117 26\n2118 26\n2119 26\n2120 26\n2121 26\n2122 26\n2123 26\n2124 26\n2125 26\n2126 26\n2127 26\n2128 26\n2129 26\n2130 26\n2131 26\n2132 26\n2133 26\n2134 26\n2135 26\n2136 26\n2137 26\n2138 26\n2139 26\n2140 26\n2141 26\n2142 26\n2143 26\n2144 26\n2145 26\n2146 26\n2147 26\n2148 26\n2149 26\n2150 26\n2151 26\n2152 26\n2153 26\n2154 26\n2155 26\n2156 26\n2157 26\n2158 26\n2159 26\n"
    }
   ],
   "source": [
    "# define indicies to split Data\n",
    "dset = ds_ear_siamese.get_dataset(data_path=Config.dataset_dir, transform_mode='size_only')\n",
    "N = len(dset)\n",
    "classes = [dset.class_to_idx[class_] for class_ in dset.classes]\n",
    "num_classes = len(classes)\n",
    "print(N)\n",
    "\n",
    "# Get amount of classes per set\n",
    "tr,va,te = percentage_split(num_classes=num_classes,\n",
    "                            train_percentage=0.6, \n",
    "                            valid_percentage=0.3,\n",
    "                            test_percentage=0.1\n",
    "                            )\n",
    "# create class separation\n",
    "train_classes = random.sample(classes, tr)\n",
    "classes = diff(classes, train_classes)\n",
    "valid_classes = random.sample(classes, va)\n",
    "classes = diff(classes, valid_classes)\n",
    "test_classes = random.sample(classes, te)\n",
    "classes = diff(classes, test_classes)\n",
    "\n",
    "train_indices, val_indices, test_indices = [],[],[]\n",
    "# go through\n",
    "for i,(_, class_idx) in enumerate(dset.imgs):\n",
    "    if class_idx in train_classes:\n",
    "        train_indices.append(i)\n",
    "        continue\n",
    "    if class_idx in valid_classes:\n",
    "        val_indices.append(i)\n",
    "        continue\n",
    "    if class_idx in test_classes:\n",
    "        test_indices.append(i)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# definde data loader\n",
    "# dl_train = ds_ear_siamese.get_dataloader(\n",
    "train_dataloader = ds_ear_siamese.get_dataloader(\n",
    "    data_path=Config.dataset_dir,\n",
    "    indices=train_indices,\n",
    "    batch_size=Config.train_batch_size,\n",
    "    num_workers=Config.num_workers,\n",
    "    transform_mode='siamese', # TODO switch to another transform?\n",
    "    should_invert = False\n",
    ")\n",
    "\n",
    "val_dataloader = ds_ear_siamese.get_dataloader(\n",
    "    data_path=Config.dataset_dir,\n",
    "    indices=val_indices,\n",
    "    batch_size=Config.val_batch_size,\n",
    "    num_workers=Config.num_workers,\n",
    "    transform_mode='siamese_valid_and_test',\n",
    "    should_invert = False\n",
    ")\n",
    "# dl_test = ds_ear_siamese.get_dataloader(\n",
    "test_dataloader = ds_ear_siamese.get_dataloader(\n",
    "    data_path=Config.dataset_dir,\n",
    "    indices=test_indices,\n",
    "    batch_size=Config.test_batch_size,\n",
    "    num_workers=Config.num_workers,\n",
    "    transform_mode='siamese_valid_and_test',\n",
    "    should_invert = False\n",
    ")\n",
    "\n",
    "vis_dataloader = ds_ear_siamese.get_dataloader(\n",
    "    data_path=Config.dataset_dir,\n",
    "    indices=train_indices,\n",
    "    batch_size=Config.vis_batch_size,\n",
    "    num_workers=Config.num_workers,\n",
    "    transform_mode='siamese',\n",
    "    should_invert = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize some data....\n",
    "dataiter = iter(vis_dataloader)\n",
    "\n",
    "example_batch = next(dataiter)\n",
    "concatenated = torch.cat((example_batch[0], example_batch[1]),0)\n",
    "imshow(make_grid(concatenated))\n",
    "print(example_batch[2].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definde Model and load to device\n",
    "if Config.NN_SIAMESE == False:\n",
    "    if Config.USE_SAVED_MODEL:\n",
    "        model = torch.load('./models/model_classification_MN_classifier_removed_96%.pt')\n",
    "    else:\n",
    "        model = mobilenet_v2(pretrained=True)\n",
    "        model.features[0][0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        #model.classifier[1] = nn.Linear(in_features=model.classifier[1].in_features, out_features=10)\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        for layer in model.features[0]:\n",
    "            layers.append(layer)\n",
    "        model.features[0][0] = nn.ReflectionPad2d(1)\n",
    "        model.features[0][1] = layers[0]\n",
    "        model.features[0][2] = layers[1]\n",
    "        model.features[0].add_module('3', layers[2])\n",
    "\n",
    "        # list(model.features[2].children())[0].add_module('4', nn.Dropout2d(0.4))\n",
    "        # list(model.features[3].children())[0].add_module('4', nn.Dropout2d(0.4))\n",
    "        # list(model.features[6].children())[0].add_module('4', nn.Dropout2d(0.4))\n",
    "        # list(model.features[8].children())[0].add_module('4', nn.Dropout2d(0.4))\n",
    "        # list(model.features[10].children())[0].add_module('4', nn.Dropout2d(0.4))\n",
    "        # list(model.features[12].children())[0].add_module('4', nn.Dropout2d(0.4))\n",
    "        # list(model.features[14].children())[0].add_module('4', nn.Dropout2d(0.4))\n",
    "        # list(model.features[16].children())[0].add_module('4', nn.Dropout2d(0.4))\n",
    "        model.classifier = nn.Sequential(*list(model.classifier.children())[:-2])\n",
    "\n",
    "\n",
    "else:\n",
    "    model = SiameseNetwork()\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "contrastive_loss_siamese = ContrastiveLoss(2.0)\n",
    "optimizer_siamese = torch.optim.Adam(model.parameters(),lr = Config.LEARNINGRATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Define which Layers we want to train\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# layers = list(model.children())[0]\n",
    "# sub_layer = list(layers.children())\n",
    "# unfreezed = [15,16,17,18]\n",
    "# for u in unfreezed:\n",
    "#     for param in sub_layer[u].parameters():\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To show trainable parameters\n",
    "from DLBio.pytorch_helpers import get_num_params\n",
    "\n",
    "get_num_params(model,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training = Training(model=model, optimizer=optimizer_siamese,train_dataloader=train_dataloader, val_dataloader=val_dataloader, loss_contrastive=contrastive_loss_siamese, nn_Siamese=Config.NN_SIAMESE, THRESHOLD=Config.TRESHOLD_VER)\n",
    "\n",
    "# epochs, loss_history, val_loss_history, acc_history, val_acc_history = training(Config.EPOCHS)\n",
    "# show_plot(epochs, loss_history, val_loss_history,'train_loss', 'val_loss',1)\n",
    "# show_plot(epochs, acc_history, val_acc_history,'train_acc', 'val_acc', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tn Bilder nicht gleich, Distanz grÃ¶Ãer als THRESH\n",
    "# fp Bilder nicht gleich, Distanz kleiner als THRESH\n",
    "# fn Bilder gleich, Distanz grÃ¶Ãer als THRESH\n",
    "# tp Bilder gleich, Distanz kleiner als THRESH\n",
    "\n",
    "def calc_test_label(thresh=Config.TRESHOLD_VER):\n",
    "    '''\n",
    "    This function processes the test dataloader and returns the true labels and the predicted labels (depending on a threshold)\n",
    "    Arguments\n",
    "    ---------\n",
    "    thresh:     Threshold for \"same-different\" classification\n",
    "                default is the Config set threshhold\n",
    "                \n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    Two lists of same length as image tuples in test loader with labels 1 or 0\n",
    "    '''\n",
    "    ground_truth_label, prediction_label = [], []\n",
    "\n",
    "    for data in test_dataloader:\n",
    "        # use training class for data extraction\n",
    "        label, output1, output2 = training.get_label_outputs(data)\n",
    "        # extend labels of the ground truth\n",
    "        ground_truth_label.extend(label.flatten().tolist())\n",
    "        # Extend the distance-threshold prediction\n",
    "        prediction_label.extend(M.batch_predictions_bin(output1, output2, thresh))\n",
    "    # lists containing all image tuple labels or predictions\n",
    "    return ground_truth_label, prediction_label\n",
    "\n",
    "ground_truth, prediction = calc_test_label(Config.TRESHOLD_VER)\n",
    "# get confusion matrix\n",
    "cf = M.cf_matrix(ground_truth, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set parameters for confusion_matrix plot\n",
    "labels = ['True Pos','False Neg','False Pos','True Neg']\n",
    "categories = ['Same', 'Different']\n",
    "\n",
    "# plot matrix\n",
    "M.make_confusion_matrix(cf,\n",
    "                        group_names=labels,\n",
    "                        categories=categories,\n",
    "                        cbar=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparation for ROC\n",
    "# define lists for the rates\n",
    "tprs = []\n",
    "fprs = []\n",
    "# Set all Thresholds to be tested\n",
    "threshholds = [x / 10 for x in list(range(1,12))]\n",
    "\n",
    "for t in threshholds:\n",
    "    ground_truth, prediction = calc_test_label(t)\n",
    "    cf = M.cf_matrix(ground_truth, prediction)\n",
    "    _,_,_,sensitivity,specificity = M.get_metrics(cf)\n",
    "\n",
    "    tprs.append(sensitivity)\n",
    "    fprs.append( (1 - specificity) )\n",
    "\n",
    "show_plot(fprs, tprs, tprs, \"ROC\", \"ROC2\", 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load('/Users/falcolentzsch/Develope/Bachelorthesis/Bachelorthesis/models/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model,'/nfshome/lentzsch/Documents/Bachelorarbeit/Bachelorthesis/models/model_MN_1.pt')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('Bachelorthesis': venv)",
   "language": "python",
   "name": "python37764bitbachelorthesisvenv2dac4a23d3734769a463ced8ddada848"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}