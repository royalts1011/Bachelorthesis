{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "import json\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch\n",
    "import torch.nn as nn\n",
    "from torch import cuda\n",
    "from torchvision.models.mobilenet import mobilenet_v2\n",
    "\n",
    "# DLBio\n",
    "from DLBio import pt_training\n",
    "from DLBio.helpers import check_mkdir\n",
    "from DLBio.pt_train_printer import Printer\n",
    "from DLBio.pytorch_helpers import get_device, get_num_params\n",
    "\n",
    "# own scripts\n",
    "import ds_ear\n",
    "from classification_test import TestMyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(pt_training.ITrainInterface):\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.xent_loss = nn.CrossEntropyLoss()\n",
    "        self.metrics = {\n",
    "            'acc': accuracy\n",
    "        }\n",
    "        self.d = device\n",
    "\n",
    "    def train_step(self, sample):\n",
    "        images, targets = sample[0].to(self.d), sample[1].to(self.d)\n",
    "        pred = self.model(images)\n",
    "\n",
    "        loss = self.xent_loss(pred, targets)\n",
    "        metrics = dict()\n",
    "\n",
    "        metrics.update({k: v(pred, targets) for k, v in self.metrics.items()})\n",
    "\n",
    "        return loss, metrics\n",
    "\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    _, y_pred = y_pred.max(1)  # grab class predictions\n",
    "    return (y_pred == y_true).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Config():\n",
    "    # influences learning rate, weight decay, data folder path and training classes\n",
    "    IS_OWN_DATASET = True\n",
    "    # define constants\n",
    "    LEARNING_RATE = (0.001, 0.01)[IS_OWN_DATASET] # FORM: (AMI, dataset)\n",
    "    WEIGHT_DECAY = (0.0001, 0.001)[IS_OWN_DATASET] # FORM: (AMI, dataset)\n",
    "    DATASET_DIR = ('../AMI', '../dataset')[IS_OWN_DATASET]\n",
    "    # grab dataset for length and classes\n",
    "    dset = ds_ear.get_dataset(data_path=DATASET_DIR, transform_mode='size_only')\n",
    "    CLASSES = len(dset.classes)\n",
    "    \n",
    "    BATCH_SIZE = 32\n",
    "    NUM_WORKERS = 3\n",
    "    EPOCHS = 15\n",
    "\n",
    "    FOLDER = './class_sample'\n",
    "    OPT_TYPE = 'Adam'\n",
    "    LR_STEPS = 3\n",
    "    DO_EARLY_STOPPING = True\n",
    "    STOP_AFTER = 10\n",
    "    ES_METRIC = 'val_acc'\n",
    "    SAVE_INTERVALL = -1\n",
    "    PRINT_FREQUENCY = 500 # print every 500 batches\n",
    "    SEED = 0\n",
    "\n",
    "# use seeds to ensure comparable results ()\n",
    "# pt_training.set_random_seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "\n",
    "model = mobilenet_v2(pretrained=True)\n",
    "fir = model.features[0][0]\n",
    "# Swap between grey and color\n",
    "#model.features[0][0] = nn.Conv2d(\n",
    "#                        in_channels=1,\n",
    "#                        out_channels=fir.out_channels,\n",
    "#                        kernel_size=fir.kernel_size,\n",
    "#                        stride=fir.stride,\n",
    "#                        padding=fir.padding,\n",
    "#                        bias=fir.bias\n",
    "#                        )\n",
    "# Remap the classification layer to the correct amount of classes\n",
    "model.classifier[1] = nn.Linear(in_features=model.classifier[1].in_features, out_features=Config.CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freezes all mobilenet parameters except classifier\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "from DLBio.pytorch_helpers import get_num_params\n",
    "get_num_params(model,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #To Define which Layers we want to train\n",
    "# feature_layers = list(model.features.children())\n",
    "# unfreezed = [18]\n",
    "# for u in unfreezed:\n",
    "#     for param in feature_layers[u].parameters():\n",
    "#         param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2283503"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "from DLBio.pytorch_helpers import get_num_params\n",
    "get_num_params(model,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cuda:0\n"
    }
   ],
   "source": [
    "device = get_device()\n",
    "# device = 'cpu'\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "# define folder to save model and log file\n",
    "check_mkdir(Config.FOLDER, is_dir=True)\n",
    "model_out = join(Config.FOLDER, 'model.pt')\n",
    "log_file = join(Config.FOLDER, 'log.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3760\n"
    }
   ],
   "source": [
    "# define indicies to split Data\n",
    "N = len(Config.dset)\n",
    "print(N)\n",
    "\n",
    "# List of index where classes switch\n",
    "class_switch = [0]\n",
    "for c in range(Config.CLASSES):\n",
    "    for i,(_, class_idx) in enumerate(Config.dset.imgs):\n",
    "        if class_idx > c:\n",
    "            # append current index of class switch\n",
    "            class_switch.append(i)\n",
    "            break\n",
    "# append last index\n",
    "class_switch.append(len(Config.dset.imgs)-1)\n",
    "\n",
    "train_indices, valid_indices, test_indices = [],[],[]\n",
    "# go through all classes\n",
    "for i in range(len(class_switch)-1):\n",
    "    # lists the indices of each class\n",
    "    rand_class = np.random.permutation(list(range(class_switch[i], class_switch[i+1])))\n",
    "    length = len(rand_class)\n",
    "    # calculate percentage of current classes\n",
    "    n_80 = int(round(.8*length))\n",
    "    n_70 = int(round(.7*length))\n",
    "    n_60 = int(round(.6*length))\n",
    "    n_20 = int(round(.2*length))\n",
    "    n_10 = int(round(.1*length))\n",
    "    # add percentage of indices to respective lists\n",
    "    train_indices.extend(rand_class[:n_70])\n",
    "    valid_indices.extend(rand_class[n_70:n_70+n_20])\n",
    "    test_indices.extend(rand_class[n_70+n_20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['alexander_bec', 'alina_sch', 'alissa_buh', 'amanda_dab', 'anna_kab', 'anni_qua', 'beatrix_mah', 'clara_pau', 'clemens_blu', 'collin_sch', 'david_fau', 'falco_len', 'felix_mec', 'gregor_spi', 'hammam_als', 'janna_qua', 'janole_pen', 'jesse_kru', 'johannes_wie', 'jule_dre', 'julia_fis', 'konrad_von', 'lars_fin', 'linus_fal', 'lynn_man', 'maike_her', 'malte_gas', 'marcel_nim', 'marcus_jue', 'marejke_wen', 'marina_fri', 'marina_han', 'matilda_kni', 'meiko_pri', 'mila_wol', 'mohammed_muh', 'moritz_bor', 'moritz_mei', 'nils_loo', 'pauline_bus', 'robert_kle', 'robert_sch', 'sarah_amo', 'sarah_feh', 'sina_jun', 'tim_moe', 'yannik_obe'] \n 47  classes\n"
    }
   ],
   "source": [
    "# write some model specs\n",
    "with open(join(Config.FOLDER, 'model_specs.json'), 'w') as file:\n",
    "    json.dump({\n",
    "        'num_trainable': float(get_num_params(model, True)),\n",
    "        'num_params': float(get_num_params(model, False))\n",
    "    }, file)\n",
    "\n",
    "# definde data loader\n",
    "dl_train = ds_ear.get_dataloader(\n",
    "    data_path=Config.DATASET_DIR,\n",
    "    indices=train_indices,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    num_workers=Config.NUM_WORKERS,\n",
    "    transform_mode='train_gray'\n",
    ")\n",
    "\n",
    "dl_valid = ds_ear.get_dataloader(\n",
    "    data_path=Config.DATASET_DIR,\n",
    "    indices=valid_indices,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    num_workers=Config.NUM_WORKERS,\n",
    "    transform_mode='valid_and_test'\n",
    ")\n",
    "\n",
    "dl_test = ds_ear.get_dataloader(\n",
    "    data_path=Config.DATASET_DIR,\n",
    "    indices=test_indices,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    num_workers=Config.NUM_WORKERS,\n",
    "    transform_mode='valid_and_test'\n",
    ")\n",
    "\n",
    "print(dl_train.dataset.classes, '\\n', len(dl_train.dataset.classes), ' classes')\n",
    "\n",
    "# with open('test_indizes.txt', 'w') as file:\n",
    "#     for idx in test_indices:\n",
    "#         file.write(\"%i\\n\" % idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "optimizer = pt_training.get_optimizer(\n",
    "        'Adam', model.parameters(),\n",
    "        Config.LEARNING_RATE,\n",
    "        weight_decay=Config.WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "if Config.LR_STEPS > 0:\n",
    "    scheduler = pt_training.get_scheduler(\n",
    "            Config.LR_STEPS, Config.EPOCHS, optimizer)\n",
    "\n",
    "if Config.DO_EARLY_STOPPING:\n",
    "    assert Config.SAVE_INTERVALL == -1\n",
    "    early_stopping = pt_training.EarlyStopping(\n",
    "            Config.ES_METRIC, get_max=True, epoch_thres=Config.STOP_AFTER\n",
    "        )\n",
    "else:\n",
    "    early_stopping = None\n",
    "    \n",
    "train_interface = Classification(model, device)\n",
    "\n",
    "training = pt_training.Training(\n",
    "        optimizer, dl_train, train_interface,\n",
    "        scheduler=scheduler, printer=Printer(Config.PRINT_FREQUENCY, log_file),\n",
    "        save_path=model_out, save_steps=Config.SAVE_INTERVALL,\n",
    "        val_data_loader=dl_valid,\n",
    "        early_stopping=early_stopping\n",
    "    )\n",
    "\n",
    "training(Config.EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"L Rate:\\t\", Config.LEARNING_RATE)\n",
    "print(\"W Dcay:\\t\", Config.WEIGHT_DECAY)\n",
    "print(\"Clas:\\t\", Config.CLASSES)\n",
    "print(\"Batch:\\t\", Config.BATCH_SIZE)\n",
    "print(\"Epochs:\\t\", Config.EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(log_file, 'r') as file:\n",
    "    log = json.load(file)\n",
    "\n",
    "plt.plot(log['acc'], label='acc')\n",
    "plt.plot(log['val_acc'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Execute this cell if you want the current variables to be added to the csv-file\n",
    "# import csv\n",
    "\n",
    "# with open(log_file, 'r') as file:\n",
    "#     log = json.load(file)\n",
    "#     ACC = log['acc']\n",
    "#     VAL_ACC = log['val_acc']\n",
    "\n",
    "# with open('var_log.csv', 'a') as f:\n",
    "#     writer = csv.writer(f, delimiter=',', lineterminator='\\n')\n",
    "#     writer.writerow([\n",
    "#         Config.DATASET_DIR.split('/')[1],        \n",
    "#         Config.LEARNING_RATE,\n",
    "#         Config.WEIGHT_DECAY,\n",
    "#         Config.CLASSES,\n",
    "#         Config.OPT_TYPE,\n",
    "#         Config.EPOCHS,\n",
    "#         Config.LR_STEPS,\n",
    "#         Config.DO_EARLY_STOPPING,\n",
    "#         Config.STOP_AFTER,\n",
    "#         Config.SEED,\n",
    "#         Config.BATCH_SIZE,\n",
    "#         Config.NUM_WORKERS,\n",
    "#         # saves a list of len(EPOCHS) with accuracies\n",
    "#         ACC,\n",
    "#         VAL_ACC,\n",
    "#         # insert optional comment\n",
    "#         \"\"\n",
    "#     ])\n",
    "# f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = TestMyModel(dl_test, Config.FOLDER)\n",
    "\n",
    "test.load_model()\n",
    "test.start_testing()\n",
    "test.class_acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.classifier = nn.Sequential(*list(model.classifier.children())[:-2])\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.save(model, './models/model_classification_MN_classifier_removed_85%.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitbachelorthesisvenvfe65d2ae6af74790a8c3d2ed63037c92",
   "display_name": "Python 3.7.7 64-bit ('Bachelorthesis': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}