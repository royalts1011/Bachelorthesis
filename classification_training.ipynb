{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "from DLBio import pt_training\n",
    "import ds_ear\n",
    "from torchvision.models.mobilenet import mobilenet_v2\n",
    "import torch.nn as nn\n",
    "from torch import cuda\n",
    "from DLBio.pytorch_helpers import get_device, get_num_params\n",
    "from DLBio.helpers import check_mkdir\n",
    "from DLBio.pt_train_printer import Printer\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(pt_training.ITrainInterface):\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.xent_loss = nn.CrossEntropyLoss()\n",
    "        self.metrics = {\n",
    "            'acc': accuracy\n",
    "        }\n",
    "        self.d = device\n",
    "\n",
    "    def train_step(self, sample):\n",
    "        images, targets = sample[0].to(self.d), sample[1].to(self.d)\n",
    "        pred = self.model(images)\n",
    "\n",
    "        loss = self.xent_loss(pred, targets)\n",
    "        metrics = dict()\n",
    "\n",
    "        metrics.update({k: v(pred, targets) for k, v in self.metrics.items()})\n",
    "\n",
    "        return loss, metrics\n",
    "\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    _, y_pred = y_pred.max(1)  # grab class predictions\n",
    "    return (y_pred == y_true).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# influences learning rate, weight decay, data folder path and training classes\n",
    "IS_OWN_DATASET = True\n",
    "\n",
    "# define constants\n",
    "LEARNING_RATE = (0.001, 0.0001)[IS_OWN_DATASET] # FORM: (AMI, dataset)\n",
    "WEIGHT_DECAY = (0.0001, 0.1)[IS_OWN_DATASET] # FORM: (AMI, dataset)\n",
    "DATASET_FOLDER = ('../AMI', '../dataset')[IS_OWN_DATASET]\n",
    "MAP_TO_CLASSES = (100, 5)[IS_OWN_DATASET]\n",
    "\n",
    "FOLDER = './class_sample'\n",
    "OPT_TYPE = 'Adam'\n",
    "EPOCHS = 10\n",
    "LR_STEPS = 3\n",
    "DO_EARLY_STOPPING = True\n",
    "STOP_AFTER = 10\n",
    "ES_METRIC = 'val_acc'\n",
    "SAVE_INTERVALL = -1\n",
    "PRINT_FREQUENCY = 500 # print every 500 batches\n",
    "SEED = 0\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 3\n",
    "\n",
    "# use seeds to ensure comparable results ()\n",
    "# pt_training.set_random_seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "\n",
    "model = mobilenet_v2(pretrained=True)\n",
    "# Remap the classification layer to the correct amount of classes\n",
    "model.classifier[1] = nn.Linear(in_features=model.classifier[1].in_features, out_features=MAP_TO_CLASSES)\n",
    "\n",
    "print(cuda.is_available())\n",
    "if cuda.is_available(): model.to('cuda:0')\n",
    "\n",
    "# define folder to save model and log file\n",
    "check_mkdir(FOLDER, is_dir=True)\n",
    "model_out = join(FOLDER, 'model.pt')\n",
    "log_file = join(FOLDER, 'log.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write some model specs\n",
    "with open(join(FOLDER, 'model_specs.json'), 'w') as file:\n",
    "    json.dump({\n",
    "        'num_trainable': float(get_num_params(model, True)),\n",
    "        'num_params': float(get_num_params(model, False))\n",
    "    }, file)\n",
    "\n",
    "# define indicies to split Data\n",
    "N = len(ds_ear.get_dataset(DATASET_FOLDER))\n",
    "n_80 = int(.8*N)\n",
    "n_60 = int(.6*N)\n",
    "n_20 = int(.2*N)\n",
    "\n",
    "rand_indeces = np.random.permutation(N)\n",
    "train_indeces = rand_indeces[:n_80]\n",
    "valid_indeces = rand_indeces[n_80:]\n",
    "#valid_indeces = rand_indeces[n_60:n_60+n_20]\n",
    "#test_indeces = rand_indeces[n_60+n_20:]\n",
    "\n",
    "# definde data loader\n",
    "dl_train = ds_ear.get_dataloader(\n",
    "    indeces=train_indeces,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    is_train=True,\n",
    "    data_path=DATASET_FOLDER\n",
    ")\n",
    "\n",
    "dl_valid = ds_ear.get_dataloader(\n",
    "    indeces=valid_indeces,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    is_train=False,\n",
    "    data_path=DATASET_FOLDER\n",
    ")\n",
    "\n",
    "# dl_test = ds_ear.get_dataloader(\n",
    "#     indeces=test_indeces,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     num_workers=NUM_WORKERS,\n",
    "#     is_train=False\n",
    "# )\n",
    "\n",
    "# with open('test_indizes.txt', 'w') as file:\n",
    "#     for idx in test_indeces:\n",
    "#         file.write(\"%i\\n\" % idx)\n",
    "\n",
    "\n",
    "# define optimizer\n",
    "optimizer = pt_training.get_optimizer(\n",
    "        'Adam', model.parameters(),\n",
    "        LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "if LR_STEPS > 0:\n",
    "    scheduler = pt_training.get_scheduler(\n",
    "            LR_STEPS, EPOCHS, optimizer)\n",
    "\n",
    "if DO_EARLY_STOPPING:\n",
    "    assert SAVE_INTERVALL == -1\n",
    "    early_stopping = pt_training.EarlyStopping(\n",
    "            ES_METRIC, get_max=True, epoch_thres=STOP_AFTER\n",
    "        )\n",
    "else:\n",
    "    early_stopping = None\n",
    "    \n",
    "train_interface = Classification(model, device)\n",
    "\n",
    "training = pt_training.Training(\n",
    "        optimizer, dl_train, train_interface,\n",
    "        scheduler=scheduler, printer=Printer(PRINT_FREQUENCY, log_file),\n",
    "        save_path=model_out, save_steps=SAVE_INTERVALL,\n",
    "        val_data_loader=dl_valid,\n",
    "        early_stopping=early_stopping\n",
    "    )\n",
    "\n",
    "training(EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(log_file, 'r') as file:\n",
    "    log = json.load(file)\n",
    "\n",
    "plt.plot(log['acc'], label='acc')\n",
    "plt.plot(log['val_acc'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell if you want the current variables to be added to the csv-file\n",
    "import csv\n",
    "\n",
    "with open(log_file, 'r') as file:\n",
    "    log = json.load(file)\n",
    "    ACC = log['acc']\n",
    "    VAL_ACC = log['val_acc']\n",
    "\n",
    "with open('var_log.csv', 'a') as f:\n",
    "    writer = csv.writer(f, delimiter=',', lineterminator='\\n')\n",
    "    writer.writerow([\n",
    "        LEARNING_RATE,\n",
    "        WEIGHT_DECAY,\n",
    "        MAP_TO_CLASSES,\n",
    "        OPT_TYPE,\n",
    "        EPOCHS,\n",
    "        LR_STEPS,\n",
    "        DO_EARLY_STOPPING,\n",
    "        STOP_AFTER,\n",
    "        SEED,\n",
    "        BATCH_SIZE,\n",
    "        NUM_WORKERS,\n",
    "        # saves a list of len(EPOCHS) with accuracies\n",
    "        ACC,\n",
    "        VAL_ACC,\n",
    "        # insert optional comment\n",
    "        \"\"\n",
    "    ])\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitbachelorthesisvenv02556f6d95bb441ca9c35a29550b23bd",
   "display_name": "Python 3.7.7 64-bit ('Bachelorthesis': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}